{"version":3,"sources":["../src/chatgpt-api.ts","../src/types.ts","../src/chatgpt-conversation.ts","../src/fetch.ts","../src/fetch-sse.ts","../src/stream-async-iterable.ts","../src/utils.ts"],"sourcesContent":["import ExpiryMap from 'expiry-map'\nimport pTimeout from 'p-timeout'\nimport { v4 as uuidv4 } from 'uuid'\n\nimport * as types from './types'\nimport { ChatGPTConversation } from './chatgpt-conversation'\nimport { fetch } from './fetch'\nimport { fetchSSE } from './fetch-sse'\nimport { markdownToText } from './utils'\n\nconst KEY_ACCESS_TOKEN = 'accessToken'\nconst USER_AGENT =\n  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n\nexport class ChatGPTAPI {\n  protected _sessionToken: string\n  protected _markdown: boolean\n  protected _apiBaseUrl: string\n  protected _backendApiBaseUrl: string\n  protected _userAgent: string\n\n  // Stores access tokens for `accessTokenTTL` milliseconds before needing to refresh\n  // (defaults to 60 seconds)\n  protected _accessTokenCache: ExpiryMap<string, string>\n\n  /**\n   * Creates a new client wrapper around the unofficial ChatGPT REST API.\n   *\n   * @param opts.sessionToken = **Required** OpenAI session token which can be found in a valid session's cookies (see readme for instructions)\n   * @param apiBaseUrl - Optional override; the base URL for ChatGPT webapp's API (`/api`)\n   * @param backendApiBaseUrl - Optional override; the base URL for the ChatGPT backend API (`/backend-api`)\n   * @param userAgent - Optional override; the `user-agent` header to use with ChatGPT requests\n   * @param accessTokenTTL - Optional override; how long in milliseconds access tokens should last before being forcefully refreshed\n   */\n  constructor(opts: {\n    sessionToken: string\n\n    /** @defaultValue `true` **/\n    markdown?: boolean\n\n    /** @defaultValue `'https://chat.openai.com/api'` **/\n    apiBaseUrl?: string\n\n    /** @defaultValue `'https://chat.openai.com/backend-api'` **/\n    backendApiBaseUrl?: string\n\n    /** @defaultValue `'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'` **/\n    userAgent?: string\n\n    /** @defaultValue 60000 (60 seconds) */\n    accessTokenTTL?: number\n  }) {\n    const {\n      sessionToken,\n      markdown = true,\n      apiBaseUrl = 'https://chat.openai.com/api',\n      backendApiBaseUrl = 'https://chat.openai.com/backend-api',\n      userAgent = USER_AGENT,\n      accessTokenTTL = 60000 // 60 seconds\n    } = opts\n\n    this._sessionToken = sessionToken\n    this._markdown = !!markdown\n    this._apiBaseUrl = apiBaseUrl\n    this._backendApiBaseUrl = backendApiBaseUrl\n    this._userAgent = userAgent\n\n    this._accessTokenCache = new ExpiryMap<string, string>(accessTokenTTL)\n\n    if (!this._sessionToken) {\n      throw new types.ChatGPTError('ChatGPT invalid session token')\n    }\n  }\n\n  /**\n   * Sends a message to ChatGPT, waits for the response to resolve, and returns\n   * the response.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   * If you want to receive the full response, including message and conversation IDs,\n   * you can use `opts.onConversationResponse` or use the `ChatGPTAPI.getConversation`\n   * helper.\n   *\n   * @param message - The prompt message to send\n   * @param opts.conversationId - Optional ID of a conversation to continue\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.onConversationResponse - Optional callback which will be invoked every time the partial response is updated with the full conversation response\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    message: string,\n    opts: types.SendMessageOptions = {}\n  ): Promise<string> {\n    const {\n      conversationId,\n      parentMessageId = uuidv4(),\n      timeoutMs,\n      onProgress,\n      onConversationResponse\n    } = opts\n\n    let { abortSignal } = opts\n\n    let abortController: AbortController = null\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController()\n      abortSignal = abortController.signal\n    }\n\n    const accessToken = await this.refreshAccessToken()\n\n    const body: types.ConversationJSONBody = {\n      action: 'next',\n      messages: [\n        {\n          id: uuidv4(),\n          role: 'user',\n          content: {\n            content_type: 'text',\n            parts: [message]\n          }\n        }\n      ],\n      model: 'text-davinci-002-render',\n      parent_message_id: parentMessageId\n    }\n\n    if (conversationId) {\n      body.conversation_id = conversationId\n    }\n\n    const url = `${this._backendApiBaseUrl}/conversation`\n    let response = ''\n\n    const responseP = new Promise<string>((resolve, reject) => {\n      fetchSSE(url, {\n        method: 'POST',\n        headers: {\n          Authorization: `Bearer ${accessToken}`,\n          'Content-Type': 'application/json',\n          'User-Agent': this._userAgent\n        },\n        body: JSON.stringify(body),\n        signal: abortSignal,\n        onMessage: (data: string) => {\n          if (data === '[DONE]') {\n            return resolve(response)\n          }\n\n          try {\n            const parsedData: types.ConversationResponseEvent = JSON.parse(data)\n            if (onConversationResponse) {\n              onConversationResponse(parsedData)\n            }\n\n            const message = parsedData.message\n            // console.log('event', JSON.stringify(parsedData, null, 2))\n\n            if (message) {\n              let text = message?.content?.parts?.[0]\n\n              if (text) {\n                if (!this._markdown) {\n                  text = markdownToText(text)\n                }\n\n                response = text\n\n                if (onProgress) {\n                  onProgress(text)\n                }\n              }\n            }\n          } catch (err) {\n            console.warn('fetchSSE onMessage unexpected error', err)\n            reject(err)\n          }\n        }\n      }).catch(reject)\n    })\n\n    if (timeoutMs) {\n      if (abortController) {\n        // This will be called when a timeout occurs in order for us to forcibly\n        // ensure that the underlying HTTP request is aborted.\n        ;(responseP as any).cancel = () => {\n          abortController.abort()\n        }\n      }\n\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: 'ChatGPT timed out waiting for response'\n      })\n    } else {\n      return responseP\n    }\n  }\n\n  /**\n   * @returns `true` if the client has a valid acces token or `false` if refreshing\n   * the token fails.\n   */\n  async getIsAuthenticated() {\n    try {\n      void (await this.refreshAccessToken())\n      return true\n    } catch (err) {\n      return false\n    }\n  }\n\n  /**\n   * Refreshes the client's access token which will succeed only if the session\n   * is still valid.\n   */\n  async ensureAuth() {\n    return await this.refreshAccessToken()\n  }\n\n  /**\n   * Attempts to refresh the current access token using the ChatGPT\n   * `sessionToken` cookie.\n   *\n   * Access tokens will be cached for up to `accessTokenTTL` milliseconds to\n   * prevent refreshing access tokens too frequently.\n   *\n   * @returns A valid access token\n   * @throws An error if refreshing the access token fails.\n   */\n  async refreshAccessToken(): Promise<string> {\n    const cachedAccessToken = this._accessTokenCache.get(KEY_ACCESS_TOKEN)\n    if (cachedAccessToken) {\n      return cachedAccessToken\n    }\n\n    let response: Response\n    try {\n      const res = await fetch('https://chat.openai.com/api/auth/session', {\n        headers: {\n          cookie: `__Secure-next-auth.session-token=${this._sessionToken}`,\n          'user-agent': this._userAgent\n        }\n      }).then((r) => {\n        response = r\n\n        if (!r.ok) {\n          const error = new types.ChatGPTError(`${r.status} ${r.statusText}`)\n          error.response = r\n          error.statusCode = r.status\n          error.statusText = r.statusText\n          throw error\n        }\n\n        return r.json() as any as types.SessionResult\n      })\n\n      const accessToken = res?.accessToken\n\n      if (!accessToken) {\n        const error = new types.ChatGPTError('Unauthorized')\n        error.response = response\n        error.statusCode = response?.status\n        error.statusText = response?.statusText\n        throw error\n      }\n\n      const appError = res?.error\n      if (appError) {\n        if (appError === 'RefreshAccessTokenError') {\n          const error = new types.ChatGPTError('session token may have expired')\n          error.response = response\n          error.statusCode = response?.status\n          error.statusText = response?.statusText\n          throw error\n        } else {\n          const error = new types.ChatGPTError(appError)\n          error.response = response\n          error.statusCode = response?.status\n          error.statusText = response?.statusText\n          throw error\n        }\n      }\n\n      this._accessTokenCache.set(KEY_ACCESS_TOKEN, accessToken)\n      return accessToken\n    } catch (err: any) {\n      const error = new types.ChatGPTError(\n        `ChatGPT failed to refresh auth token. ${err.toString()}`\n      )\n      error.response = response\n      error.statusCode = response?.status\n      error.statusText = response?.statusText\n      error.originalError = err\n      throw error\n    }\n  }\n\n  /**\n   * Gets a new ChatGPTConversation instance, which can be used to send multiple\n   * messages as part of a single conversation.\n   *\n   * @param opts.conversationId - Optional ID of the previous message in a conversation\n   * @param opts.parentMessageId - Optional ID of the previous message in a conversation\n   * @returns The new conversation instance\n   */\n  getConversation(\n    opts: { conversationId?: string; parentMessageId?: string } = {}\n  ) {\n    return new ChatGPTConversation(this, opts)\n  }\n}\n","export type ContentType = 'text'\n\nexport type Role = 'user' | 'assistant'\n\n/**\n * https://chat.openapi.com/api/auth/session\n */\nexport type SessionResult = {\n  /**\n   * Authenticated user\n   */\n  user: User\n\n  /**\n   * ISO date of the expiration date of the access token\n   */\n  expires: string\n\n  /**\n   * The access token\n   */\n  accessToken: string\n\n  /**\n   * If there was an error associated with this request\n   */\n  error?: string | null\n}\n\nexport type User = {\n  /**\n   * ID of the user\n   */\n  id: string\n\n  /**\n   * Name of the user\n   */\n  name: string\n\n  /**\n   * Email of the user\n   */\n  email: string\n\n  /**\n   * Image of the user\n   */\n  image: string\n\n  /**\n   * Picture of the user\n   */\n  picture: string\n\n  /**\n   * Groups the user is in\n   */\n  groups: string[] | []\n\n  /**\n   * Features the user is in\n   */\n  features: string[] | []\n}\n\n/**\n * https://chat.openapi.com/backend-api/models\n */\nexport type ModelsResult = {\n  /**\n   * Array of models\n   */\n  models: Model[]\n}\n\nexport type Model = {\n  /**\n   * Name of the model\n   */\n  slug: string\n\n  /**\n   * Max tokens of the model\n   */\n  max_tokens: number\n\n  /**\n   * Whether or not the model is special\n   */\n  is_special: boolean\n}\n\n/**\n * https://chat.openapi.com/backend-api/moderations\n */\nexport type ModerationsJSONBody = {\n  /**\n   * Input for the moderation decision\n   */\n  input: string\n\n  /**\n   * The model to use in the decision\n   */\n  model: AvailableModerationModels\n}\n\nexport type AvailableModerationModels = 'text-moderation-playground'\n\n/**\n * https://chat.openapi.com/backend-api/moderations\n */\nexport type ModerationsJSONResult = {\n  /**\n   * Whether or not the input is flagged\n   */\n  flagged: boolean\n\n  /**\n   * Whether or not the input is blocked\n   */\n  blocked: boolean\n\n  /**\n   * The ID of the decision\n   */\n  moderation_id: string\n}\n\n/**\n * https://chat.openapi.com/backend-api/conversation\n */\nexport type ConversationJSONBody = {\n  /**\n   * The action to take\n   */\n  action: string\n\n  /**\n   * The ID of the conversation\n   */\n  conversation_id?: string\n\n  /**\n   * Prompts to provide\n   */\n  messages: Prompt[]\n\n  /**\n   * The model to use\n   */\n  model: string\n\n  /**\n   * The parent message ID\n   */\n  parent_message_id: string\n}\n\nexport type Prompt = {\n  /**\n   * The content of the prompt\n   */\n  content: PromptContent\n\n  /**\n   * The ID of the prompt\n   */\n  id: string\n\n  /**\n   * The role played in the prompt\n   */\n  role: Role\n}\n\nexport type PromptContent = {\n  /**\n   * The content type of the prompt\n   */\n  content_type: ContentType\n\n  /**\n   * The parts to the prompt\n   */\n  parts: string[]\n}\n\n/**\n * https://chat.openapi.com/backend-api/conversation/message_feedback\n */\nexport type MessageFeedbackJSONBody = {\n  /**\n   * The ID of the conversation\n   */\n  conversation_id: string\n\n  /**\n   * The message ID\n   */\n  message_id: string\n\n  /**\n   * The rating\n   */\n  rating: MessageFeedbackRating\n\n  /**\n   * Tags to give the rating\n   */\n  tags?: MessageFeedbackTags[]\n\n  /**\n   * The text to include\n   */\n  text?: string\n}\n\nexport type MessageFeedbackTags = 'harmful' | 'false' | 'not-helpful'\n\nexport type MessageFeedbackResult = {\n  /**\n   * The message ID\n   */\n  message_id: string\n\n  /**\n   * The ID of the conversation\n   */\n  conversation_id: string\n\n  /**\n   * The ID of the user\n   */\n  user_id: string\n\n  /**\n   * The rating\n   */\n  rating: MessageFeedbackRating\n\n  /**\n   * The text the server received, including tags\n   */\n  text?: string\n}\n\nexport type MessageFeedbackRating = 'thumbsUp' | 'thumbsDown'\n\nexport type ConversationResponseEvent = {\n  message?: Message\n  conversation_id?: string\n  error?: string | null\n}\n\nexport type Message = {\n  id: string\n  content: MessageContent\n  role: string\n  user: string | null\n  create_time: string | null\n  update_time: string | null\n  end_turn: null\n  weight: number\n  recipient: string\n  metadata: MessageMetadata\n}\n\nexport type MessageContent = {\n  content_type: string\n  parts: string[]\n}\n\nexport type MessageMetadata = any\n\nexport type SendMessageOptions = {\n  conversationId?: string\n  parentMessageId?: string\n  timeoutMs?: number\n  onProgress?: (partialResponse: string) => void\n  onConversationResponse?: (response: ConversationResponseEvent) => void\n  abortSignal?: AbortSignal\n}\n\nexport type SendConversationMessageOptions = Omit<\n  SendMessageOptions,\n  'conversationId' | 'parentMessageId'\n>\n\nexport class ChatGPTError extends Error {\n  statusCode?: number\n  statusText?: string\n  response?: Response\n  originalError?: Error\n}\n","import * as types from './types'\nimport { type ChatGPTAPI } from './chatgpt-api'\n\n/**\n * A conversation wrapper around the ChatGPTAPI. This allows you to send\n * multiple messages to ChatGPT and receive responses, without having to\n * manually pass the conversation ID and parent message ID for each message.\n */\nexport class ChatGPTConversation {\n  api: ChatGPTAPI\n  conversationId: string = undefined\n  parentMessageId: string = undefined\n\n  /**\n   * Creates a new conversation wrapper around the ChatGPT API.\n   *\n   * @param api - The ChatGPT API instance to use\n   * @param opts.conversationId - Optional ID of a conversation to continue\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation\n   */\n  constructor(\n    api: ChatGPTAPI,\n    opts: { conversationId?: string; parentMessageId?: string } = {}\n  ) {\n    this.api = api\n    this.conversationId = opts.conversationId\n    this.parentMessageId = opts.parentMessageId\n  }\n\n  /**\n   * Sends a message to ChatGPT, waits for the response to resolve, and returns\n   * the response.\n   *\n   * If this is the first message in the conversation, the conversation ID and\n   * parent message ID will be automatically set.\n   *\n   * This allows you to send multiple messages to ChatGPT and receive responses,\n   * without having to manually pass the conversation ID and parent message ID\n   * for each message.\n   *\n   * @param message - The prompt message to send\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.onConversationResponse - Optional callback which will be invoked every time the partial response is updated with the full conversation response\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    message: string,\n    opts: types.SendConversationMessageOptions = {}\n  ): Promise<string> {\n    const { onConversationResponse, ...rest } = opts\n\n    return this.api.sendMessage(message, {\n      ...rest,\n      conversationId: this.conversationId,\n      parentMessageId: this.parentMessageId,\n      onConversationResponse: (response) => {\n        if (response.conversation_id) {\n          this.conversationId = response.conversation_id\n        }\n\n        if (response.message?.id) {\n          this.parentMessageId = response.message.id\n        }\n\n        if (onConversationResponse) {\n          return onConversationResponse(response)\n        }\n      }\n    })\n  }\n}\n","/// <reference lib=\"dom\" />\n\nlet _undici: any\n\n// Use `undici` for node.js 16 and 17\n// Use `fetch` for node.js >= 18\n// Use `fetch` for all other environments, including browsers\n// NOTE: The top-level await is removed in a `postbuild` npm script for the\n// browser build\nconst fetch =\n  globalThis.fetch ??\n  async function undiciFetchWrapper(\n    ...args: Parameters<typeof globalThis.fetch>\n  ): Promise<Response> {\n    if (!_undici) {\n      _undici = await import('undici')\n    }\n\n    if (typeof _undici?.fetch !== 'function') {\n      throw new Error(\n        'Invalid undici installation; please make sure undici is installed correctly in your node_modules. Note that this package requires Node.js >= 16.8'\n      )\n    }\n\n    return _undici.fetch(...args)\n  }\n\nexport { fetch }\n","import { createParser } from 'eventsource-parser'\n\nimport * as types from './types'\nimport { fetch } from './fetch'\nimport { streamAsyncIterable } from './stream-async-iterable'\n\nexport async function fetchSSE(\n  url: string,\n  options: Parameters<typeof fetch>[1] & { onMessage: (data: string) => void }\n) {\n  const { onMessage, ...fetchOptions } = options\n  const res = await fetch(url, fetchOptions)\n  if (!res.ok) {\n    const msg = `ChatGPTAPI error ${res.status || res.statusText}`\n    const error = new types.ChatGPTError(msg)\n    error.statusCode = res.status\n    error.statusText = res.statusText\n    error.response = res\n    throw error\n  }\n\n  const parser = createParser((event) => {\n    if (event.type === 'event') {\n      onMessage(event.data)\n    }\n  })\n\n  if (!res.body.getReader) {\n    // Vercel polyfills `fetch` with `node-fetch`, which doesn't conform to\n    // web standards, so this is a workaround...\n    const body: NodeJS.ReadableStream = res.body as any\n\n    if (!body.on || !body.read) {\n      throw new types.ChatGPTError('unsupported \"fetch\" implementation')\n    }\n\n    body.on('readable', () => {\n      let chunk: string | Buffer\n      while (null !== (chunk = body.read())) {\n        parser.feed(chunk.toString())\n      }\n    })\n  } else {\n    for await (const chunk of streamAsyncIterable(res.body)) {\n      const str = new TextDecoder().decode(chunk)\n      parser.feed(str)\n    }\n  }\n}\n","export async function* streamAsyncIterable<T>(stream: ReadableStream<T>) {\n  const reader = stream.getReader()\n  try {\n    while (true) {\n      const { done, value } = await reader.read()\n      if (done) {\n        return\n      }\n      yield value\n    }\n  } finally {\n    reader.releaseLock()\n  }\n}\n","import { remark } from 'remark'\nimport stripMarkdown from 'strip-markdown'\n\nexport function markdownToText(markdown?: string): string {\n  return remark()\n    .use(stripMarkdown)\n    .processSync(markdown ?? '')\n    .toString()\n}\n"],"mappings":";AAAA,OAAO,eAAe;AACtB,OAAO,cAAc;AACrB,SAAS,MAAM,cAAc;;;ACgStB,IAAM,eAAN,cAA2B,MAAM;AAKxC;;;AC/RO,IAAM,sBAAN,MAA0B;AAAA,EAY/B,YACE,KACA,OAA8D,CAAC,GAC/D;AAbF,0BAAyB;AACzB,2BAA0B;AAaxB,SAAK,MAAM;AACX,SAAK,iBAAiB,KAAK;AAC3B,SAAK,kBAAkB,KAAK;AAAA,EAC9B;AAAA,EAoBA,MAAM,YACJ,SACA,OAA6C,CAAC,GAC7B;AACjB,UAAM,EAAE,2BAA2B,KAAK,IAAI;AAE5C,WAAO,KAAK,IAAI,YAAY,SAAS;AAAA,MACnC,GAAG;AAAA,MACH,gBAAgB,KAAK;AAAA,MACrB,iBAAiB,KAAK;AAAA,MACtB,wBAAwB,CAAC,aAAa;AAzD5C;AA0DQ,YAAI,SAAS,iBAAiB;AAC5B,eAAK,iBAAiB,SAAS;AAAA,QACjC;AAEA,aAAI,cAAS,YAAT,mBAAkB,IAAI;AACxB,eAAK,kBAAkB,SAAS,QAAQ;AAAA,QAC1C;AAEA,YAAI,wBAAwB;AAC1B,iBAAO,uBAAuB,QAAQ;AAAA,QACxC;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACtEA,IAAI;AAOJ,IAAM,QACJ,WAAW,SACX,eAAe,sBACV,MACgB;AACnB,MAAI,CAAC,SAAS;AACZ,cAAU,MAAM,OAAO;AAAA,EACzB;AAEA,MAAI,QAAO,mCAAS,WAAU,YAAY;AACxC,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,SAAO,QAAQ,MAAM,GAAG,IAAI;AAC9B;;;ACzBF,SAAS,oBAAoB;;;ACA7B,gBAAuB,oBAAuB,QAA2B;AACvE,QAAM,SAAS,OAAO,UAAU;AAChC,MAAI;AACF,WAAO,MAAM;AACX,YAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,UAAI,MAAM;AACR;AAAA,MACF;AACA,YAAM;AAAA,IACR;AAAA,EACF,UAAE;AACA,WAAO,YAAY;AAAA,EACrB;AACF;;;ADPA,eAAsB,SACpB,KACA,SACA;AACA,QAAM,EAAE,cAAc,aAAa,IAAI;AACvC,QAAM,MAAM,MAAM,MAAM,KAAK,YAAY;AACzC,MAAI,CAAC,IAAI,IAAI;AACX,UAAM,MAAM,oBAAoB,IAAI,UAAU,IAAI;AAClD,UAAM,QAAQ,IAAU,aAAa,GAAG;AACxC,UAAM,aAAa,IAAI;AACvB,UAAM,aAAa,IAAI;AACvB,UAAM,WAAW;AACjB,UAAM;AAAA,EACR;AAEA,QAAM,SAAS,aAAa,CAAC,UAAU;AACrC,QAAI,MAAM,SAAS,SAAS;AAC1B,gBAAU,MAAM,IAAI;AAAA,IACtB;AAAA,EACF,CAAC;AAED,MAAI,CAAC,IAAI,KAAK,WAAW;AAGvB,UAAM,OAA8B,IAAI;AAExC,QAAI,CAAC,KAAK,MAAM,CAAC,KAAK,MAAM;AAC1B,YAAM,IAAU,aAAa,oCAAoC;AAAA,IACnE;AAEA,SAAK,GAAG,YAAY,MAAM;AACxB,UAAI;AACJ,aAAO,UAAU,QAAQ,KAAK,KAAK,IAAI;AACrC,eAAO,KAAK,MAAM,SAAS,CAAC;AAAA,MAC9B;AAAA,IACF,CAAC;AAAA,EACH,OAAO;AACL,qBAAiB,SAAS,oBAAoB,IAAI,IAAI,GAAG;AACvD,YAAM,MAAM,IAAI,YAAY,EAAE,OAAO,KAAK;AAC1C,aAAO,KAAK,GAAG;AAAA,IACjB;AAAA,EACF;AACF;;;AEhDA,SAAS,cAAc;AACvB,OAAO,mBAAmB;AAEnB,SAAS,eAAe,UAA2B;AACxD,SAAO,OAAO,EACX,IAAI,aAAa,EACjB,YAAY,YAAY,EAAE,EAC1B,SAAS;AACd;;;ANEA,IAAM,mBAAmB;AACzB,IAAM,aACJ;AAEK,IAAM,aAAN,MAAiB;AAAA,EAoBtB,YAAY,MAiBT;AACD,UAAM;AAAA,MACJ;AAAA,MACA,WAAW;AAAA,MACX,aAAa;AAAA,MACb,oBAAoB;AAAA,MACpB,YAAY;AAAA,MACZ,iBAAiB;AAAA,IACnB,IAAI;AAEJ,SAAK,gBAAgB;AACrB,SAAK,YAAY,CAAC,CAAC;AACnB,SAAK,cAAc;AACnB,SAAK,qBAAqB;AAC1B,SAAK,aAAa;AAElB,SAAK,oBAAoB,IAAI,UAA0B,cAAc;AAErE,QAAI,CAAC,KAAK,eAAe;AACvB,YAAM,IAAU,aAAa,+BAA+B;AAAA,IAC9D;AAAA,EACF;AAAA,EAqBA,MAAM,YACJ,SACA,OAAiC,CAAC,GACjB;AACjB,UAAM;AAAA,MACJ;AAAA,MACA,kBAAkB,OAAO;AAAA,MACzB;AAAA,MACA;AAAA,MACA;AAAA,IACF,IAAI;AAEJ,QAAI,EAAE,YAAY,IAAI;AAEtB,QAAI,kBAAmC;AACvC,QAAI,aAAa,CAAC,aAAa;AAC7B,wBAAkB,IAAI,gBAAgB;AACtC,oBAAc,gBAAgB;AAAA,IAChC;AAEA,UAAM,cAAc,MAAM,KAAK,mBAAmB;AAElD,UAAM,OAAmC;AAAA,MACvC,QAAQ;AAAA,MACR,UAAU;AAAA,QACR;AAAA,UACE,IAAI,OAAO;AAAA,UACX,MAAM;AAAA,UACN,SAAS;AAAA,YACP,cAAc;AAAA,YACd,OAAO,CAAC,OAAO;AAAA,UACjB;AAAA,QACF;AAAA,MACF;AAAA,MACA,OAAO;AAAA,MACP,mBAAmB;AAAA,IACrB;AAEA,QAAI,gBAAgB;AAClB,WAAK,kBAAkB;AAAA,IACzB;AAEA,UAAM,MAAM,GAAG,KAAK;AACpB,QAAI,WAAW;AAEf,UAAM,YAAY,IAAI,QAAgB,CAAC,SAAS,WAAW;AACzD,eAAS,KAAK;AAAA,QACZ,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,eAAe,UAAU;AAAA,UACzB,gBAAgB;AAAA,UAChB,cAAc,KAAK;AAAA,QACrB;AAAA,QACA,MAAM,KAAK,UAAU,IAAI;AAAA,QACzB,QAAQ;AAAA,QACR,WAAW,CAAC,SAAiB;AApJrC;AAqJU,cAAI,SAAS,UAAU;AACrB,mBAAO,QAAQ,QAAQ;AAAA,UACzB;AAEA,cAAI;AACF,kBAAM,aAA8C,KAAK,MAAM,IAAI;AACnE,gBAAI,wBAAwB;AAC1B,qCAAuB,UAAU;AAAA,YACnC;AAEA,kBAAMA,WAAU,WAAW;AAG3B,gBAAIA,UAAS;AACX,kBAAI,QAAO,WAAAA,YAAA,gBAAAA,SAAS,YAAT,mBAAkB,UAAlB,mBAA0B;AAErC,kBAAI,MAAM;AACR,oBAAI,CAAC,KAAK,WAAW;AACnB,yBAAO,eAAe,IAAI;AAAA,gBAC5B;AAEA,2BAAW;AAEX,oBAAI,YAAY;AACd,6BAAW,IAAI;AAAA,gBACjB;AAAA,cACF;AAAA,YACF;AAAA,UACF,SAAS,KAAP;AACA,oBAAQ,KAAK,uCAAuC,GAAG;AACvD,mBAAO,GAAG;AAAA,UACZ;AAAA,QACF;AAAA,MACF,CAAC,EAAE,MAAM,MAAM;AAAA,IACjB,CAAC;AAED,QAAI,WAAW;AACb,UAAI,iBAAiB;AAGnB;AAAC,QAAC,UAAkB,SAAS,MAAM;AACjC,0BAAgB,MAAM;AAAA,QACxB;AAAA,MACF;AAEA,aAAO,SAAS,WAAW;AAAA,QACzB,cAAc;AAAA,QACd,SAAS;AAAA,MACX,CAAC;AAAA,IACH,OAAO;AACL,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAMA,MAAM,qBAAqB;AACzB,QAAI;AACF,WAAM,MAAM,KAAK,mBAAmB;AACpC,aAAO;AAAA,IACT,SAAS,KAAP;AACA,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAMA,MAAM,aAAa;AACjB,WAAO,MAAM,KAAK,mBAAmB;AAAA,EACvC;AAAA,EAYA,MAAM,qBAAsC;AAC1C,UAAM,oBAAoB,KAAK,kBAAkB,IAAI,gBAAgB;AACrE,QAAI,mBAAmB;AACrB,aAAO;AAAA,IACT;AAEA,QAAI;AACJ,QAAI;AACF,YAAM,MAAM,MAAM,MAAM,4CAA4C;AAAA,QAClE,SAAS;AAAA,UACP,QAAQ,oCAAoC,KAAK;AAAA,UACjD,cAAc,KAAK;AAAA,QACrB;AAAA,MACF,CAAC,EAAE,KAAK,CAAC,MAAM;AACb,mBAAW;AAEX,YAAI,CAAC,EAAE,IAAI;AACT,gBAAM,QAAQ,IAAU,aAAa,GAAG,EAAE,UAAU,EAAE,YAAY;AAClE,gBAAM,WAAW;AACjB,gBAAM,aAAa,EAAE;AACrB,gBAAM,aAAa,EAAE;AACrB,gBAAM;AAAA,QACR;AAEA,eAAO,EAAE,KAAK;AAAA,MAChB,CAAC;AAED,YAAM,cAAc,2BAAK;AAEzB,UAAI,CAAC,aAAa;AAChB,cAAM,QAAQ,IAAU,aAAa,cAAc;AACnD,cAAM,WAAW;AACjB,cAAM,aAAa,qCAAU;AAC7B,cAAM,aAAa,qCAAU;AAC7B,cAAM;AAAA,MACR;AAEA,YAAM,WAAW,2BAAK;AACtB,UAAI,UAAU;AACZ,YAAI,aAAa,2BAA2B;AAC1C,gBAAM,QAAQ,IAAU,aAAa,gCAAgC;AACrE,gBAAM,WAAW;AACjB,gBAAM,aAAa,qCAAU;AAC7B,gBAAM,aAAa,qCAAU;AAC7B,gBAAM;AAAA,QACR,OAAO;AACL,gBAAM,QAAQ,IAAU,aAAa,QAAQ;AAC7C,gBAAM,WAAW;AACjB,gBAAM,aAAa,qCAAU;AAC7B,gBAAM,aAAa,qCAAU;AAC7B,gBAAM;AAAA,QACR;AAAA,MACF;AAEA,WAAK,kBAAkB,IAAI,kBAAkB,WAAW;AACxD,aAAO;AAAA,IACT,SAAS,KAAP;AACA,YAAM,QAAQ,IAAU;AAAA,QACtB,yCAAyC,IAAI,SAAS;AAAA,MACxD;AACA,YAAM,WAAW;AACjB,YAAM,aAAa,qCAAU;AAC7B,YAAM,aAAa,qCAAU;AAC7B,YAAM,gBAAgB;AACtB,YAAM;AAAA,IACR;AAAA,EACF;AAAA,EAUA,gBACE,OAA8D,CAAC,GAC/D;AACA,WAAO,IAAI,oBAAoB,MAAM,IAAI;AAAA,EAC3C;AACF;","names":["message"]}